{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load colmap DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Dict, Tuple\n",
    "import os\n",
    "from collections import defaultdict\n",
    "os.chdir(\"..\")\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "from colmap_database import (\n",
    "    COLMAPDatabase, array_to_blob, blob_to_array, image_ids_to_pair_id, pair_id_to_image_ids\n",
    ")\n",
    "from loftr.utils.plotting import make_matching_figure\n",
    "from loftr.loftr import LoFTR, default_cfg\n",
    "from paralleldomain import AnyPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"/home/michael/datasets/Tokyo_Ginza_alley_ham_store_12_16/distorted/database.db\"\n",
    "db = COLMAPDatabase.connect(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_map = db.execute(\"SELECT image_id, name FROM images\").fetchall()\n",
    "# image_ids = [im[0] for im in image_map]\n",
    "# image_names = [im[1] for im in image_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = AnyPath(\"/home/michael/datasets/Tokyo_Ginza_alley_ham_store_12_16/originalImages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image_names(image_map):\n",
    "    for idx,(img0_id, img0_name) in enumerate(image_map):\n",
    "        for img1_id, img1_name in image_map[idx+1:]:\n",
    "            pair_id = image_ids_to_pair_id(img0_id,img1_id)\n",
    "            yield pair_id, img0_id, img0_name, img1_id, img1_name\n",
    "\n",
    "def preprocess_images(img_path: Union[AnyPath,str], img0_name: Union[AnyPath,str], img1_name: Union[AnyPath,str], dims=(768,512)) -> Dict[str,torch.Tensor]:\n",
    "    img0_pth = AnyPath(img_path) / img0_name\n",
    "    img1_pth = AnyPath(img_path) / img1_name\n",
    "    img0_raw = cv2.resize(cv2.imread(str(img0_pth), cv2.IMREAD_GRAYSCALE),dims)\n",
    "    img1_raw = cv2.resize(cv2.imread(str(img1_pth), cv2.IMREAD_GRAYSCALE),dims)\n",
    "    # Check that input size is divisible by 8\n",
    "    img0_raw = cv2.resize(img0_raw, (img0_raw.shape[1]//8*8, img0_raw.shape[0]//8*8))\n",
    "    img1_raw = cv2.resize(img1_raw, (img1_raw.shape[1]//8*8, img1_raw.shape[0]//8*8))\n",
    "\n",
    "    img0 = torch.from_numpy(img0_raw)[None][None].cuda() / 255.\n",
    "    img1 = torch.from_numpy(img1_raw)[None][None].cuda() / 255.\n",
    "    batch = {'image0': img0, 'image1': img1}\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "# The default config uses dual-softmax.\n",
    "# The outdoor and indoor models share the same config.\n",
    "# You can change the default values like thr and coarse_match_type.\n",
    "matcher = LoFTR(config=default_cfg)\n",
    "matcher.load_state_dict(torch.load(\"weights/outdoor_ds.ckpt\")['state_dict'])\n",
    "matcher = matcher.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_blob_dict = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: rescale to full images?\n",
    "count = 0\n",
    "for pair_id, img0_id, img0_name, img1_id, img1_name in parse_image_names(image_map):\n",
    "\n",
    "    batch = preprocess_images(img_path=image_path, img0_name=img0_name, img1_name=img1_name)\n",
    "    # Inference with LoFTR and get prediction\n",
    "    with torch.no_grad():\n",
    "        matcher(batch)\n",
    "        mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "        mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "        # mconf = batch['mconf'].cpu().numpy()\n",
    "    \n",
    "    match_table_entry = []\n",
    "    idx0 = 0\n",
    "    idx1 = 0\n",
    "    for coords0,coords1 in zip(mkpts0.astype(int),mkpts1.astype(int)):\n",
    "        # Each keypoint is added to keypoint_blob_dict[img_id].\n",
    "        # The point's index in that list is entered into the matches table.\n",
    "        feat_idx0 = keypoint_blob_dict[img0_id].setdefault(tuple(coords0),idx0)\n",
    "        feat_idx1 = keypoint_blob_dict[img1_id].setdefault(tuple(coords1),idx1)\n",
    "        if idx0 == feat_idx0:\n",
    "            idx0 += 1\n",
    "        if idx1 == feat_idx1:\n",
    "            idx1 += 1\n",
    "        # Add to matching table blob\n",
    "        match_table_entry.append([feat_idx0,feat_idx1])\n",
    "    # Write matching blob to matches table\n",
    "    db.execute(\n",
    "            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n",
    "            (pair_id,) + mkpts0.shape + (array_to_blob(np.array(match_table_entry)),\n",
    "            ),)\n",
    "    count += 1\n",
    "    if count == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the keypoint table\n",
    "for image_id,keypoint_dict in keypoint_blob_dict.items():\n",
    "    keypoints = np.array(list(keypoint_dict.keys()))\n",
    "    db.execute(\n",
    "            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + keypoints.shape + (array_to_blob(keypoints),),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[510,  97],\n",
       "       [496, 297],\n",
       "       [353, 231],\n",
       "       [736,  24],\n",
       "       [294,  40],\n",
       "       [ 33, 224],\n",
       "       [ 48, 231],\n",
       "       [ 17, 225],\n",
       "       [ 32, 231],\n",
       "       [ 40, 232],\n",
       "       [ 47, 239],\n",
       "       [303, 238],\n",
       "       [321, 229],\n",
       "       [313, 223],\n",
       "       [474, 185],\n",
       "       [192,  71],\n",
       "       [ 16, 233],\n",
       "       [ 39, 242],\n",
       "       [ 46, 245],\n",
       "       [673, 344],\n",
       "       [344, 280],\n",
       "       [311, 240],\n",
       "       [472, 208],\n",
       "       [128,  64],\n",
       "       [ 15, 240],\n",
       "       [ 55, 256],\n",
       "       [688, 351],\n",
       "       [350, 286],\n",
       "       [392, 240],\n",
       "       [110, 423],\n",
       "       [ 34, 273],\n",
       "       [335, 262],\n",
       "       [ 33, 281],\n",
       "       [ 48, 286],\n",
       "       [ 96, 447],\n",
       "       [690, 336],\n",
       "       [ 17, 285],\n",
       "       [ 24, 286],\n",
       "       [ 32, 288],\n",
       "       [ 32, 438],\n",
       "       [ 39, 439],\n",
       "       [ 65, 288],\n",
       "       [ 32, 296],\n",
       "       [ 55, 304],\n",
       "       [ 22, 303],\n",
       "       [ 31, 304],\n",
       "       [ 39, 305],\n",
       "       [ 55, 311],\n",
       "       [439, 376],\n",
       "       [455, 375],\n",
       "       [736, 407],\n",
       "       [519, 215],\n",
       "       [ 14, 310],\n",
       "       [ 22, 311],\n",
       "       [ 30, 313],\n",
       "       [ 55, 319],\n",
       "       [ 72, 321],\n",
       "       [ 32, 471],\n",
       "       [455, 391],\n",
       "       [511, 488],\n",
       "       [ 19, 319],\n",
       "       [ 29, 320],\n",
       "       [ 37, 320],\n",
       "       [ 53, 326],\n",
       "       [ 71, 328],\n",
       "       [ 32, 480],\n",
       "       [ 39, 479],\n",
       "       [573, 448],\n",
       "       [471, 383],\n",
       "       [567, 400],\n",
       "       [215, 440],\n",
       "       [464, 454],\n",
       "       [ 18, 328],\n",
       "       [ 27, 328],\n",
       "       [ 42, 333],\n",
       "       [ 50, 333],\n",
       "       [ 57, 335],\n",
       "       [ 64, 336],\n",
       "       [656, 327],\n",
       "       [584, 409],\n",
       "       [432, 369],\n",
       "       [698, 360],\n",
       "       [478, 479],\n",
       "       [ 18, 337],\n",
       "       [ 26, 337],\n",
       "       [ 49, 341],\n",
       "       [ 56, 342],\n",
       "       [ 41, 431],\n",
       "       [ 17, 345],\n",
       "       [ 25, 350],\n",
       "       [ 33, 350],\n",
       "       [ 41, 350],\n",
       "       [ 48, 351],\n",
       "       [ 56, 351],\n",
       "       [ 16, 357],\n",
       "       [ 23, 358],\n",
       "       [ 32, 357],\n",
       "       [ 40, 359],\n",
       "       [ 47, 358],\n",
       "       [ 55, 360],\n",
       "       [344, 424],\n",
       "       [ 14, 365],\n",
       "       [ 23, 366],\n",
       "       [ 31, 366],\n",
       "       [ 40, 366],\n",
       "       [ 47, 366],\n",
       "       [ 54, 367],\n",
       "       [ 39, 375],\n",
       "       [ 46, 375],\n",
       "       [ 54, 375],\n",
       "       [343, 441],\n",
       "       [ 38, 382],\n",
       "       [ 45, 383],\n",
       "       [ 59, 384],\n",
       "       [ 66, 384],\n",
       "       [497, 431],\n",
       "       [ 37, 391],\n",
       "       [ 51, 392],\n",
       "       [ 65, 392],\n",
       "       [341, 431],\n",
       "       [ 58, 401],\n",
       "       [ 64, 401],\n",
       "       [ 54, 463],\n",
       "       [ 64, 409],\n",
       "       [696, 488],\n",
       "       [ 33, 418],\n",
       "       [ 41, 418],\n",
       "       [ 49, 418],\n",
       "       [ 57, 417],\n",
       "       [ 64, 418],\n",
       "       [ 33, 425],\n",
       "       [ 48, 427],\n",
       "       [ 55, 427],\n",
       "       [ 63, 426]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: round matches to nearest pixel? \n",
    "# keypoint entry for img 1\n",
    "np.array(list(keypoint_dict.keys())).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109.0, 1187)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_id_to_image_ids(pair_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_arrr = blob_to_array(match_blob,dtype=np.int64,shape=(match_rows,match_cols))\n",
    "keypt_arr = blob_to_array(keypt_blob,dtype=np.int64,shape=(keypt_rows,keypt_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0],\n",
       "       [   0,    1],\n",
       "       [   1,    2],\n",
       "       ...,\n",
       "       [1620, 3530],\n",
       "       [1621, 3531],\n",
       "       [1622, 3532]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows,cols,match_blob = db.execute(\"SELECT rows,cols,data from matches WHERE pair_id=?\",(pair_id,)).fetchone()\n",
    "blob_to_array(match_blob,dtype=np.int64,shape=(match_rows,match_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0],\n",
       "       [   0,    1],\n",
       "       [   1,    2],\n",
       "       ...,\n",
       "       [1620, 3530],\n",
       "       [1621, 3531],\n",
       "       [1622, 3532]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_arrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_id, match_rows, match_cols, match_blob = db.execute(\"SELECT * FROM matches;\").fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id, keypt_rows, keypt_cols, keypt_blob = db.execute(\"SELECT * FROM keypoints;\").fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"PRAGMA table_info('matches')\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = mkpts0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indoor Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loftr.loftr import LoFTR, default_cfg\n",
    "\n",
    "# The default config uses dual-softmax.\n",
    "# The outdoor and indoor models share the same config.\n",
    "# You can change the default values like thr and coarse_match_type.\n",
    "_default_cfg = deepcopy(default_cfg)\n",
    "_default_cfg['coarse']['temp_bug_fix'] = True  # set to False when using the old ckpt\n",
    "matcher = LoFTR(config=_default_cfg)\n",
    "matcher.load_state_dict(torch.load(\"weights/indoor_ds_new.ckpt\")['state_dict'])\n",
    "matcher = matcher.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load example images\n",
    "img0_pth = img_path+\n",
    "img1_pth = \"assets/scannet_sample_images/scene0711_00_frame-001995.jpg\"\n",
    "\n",
    "img0_raw = cv2.imread(img0_pth, cv2.IMREAD_GRAYSCALE)\n",
    "img1_raw = cv2.imread(img1_pth, cv2.IMREAD_GRAYSCALE)\n",
    "img0_raw = cv2.resize(img0_raw, (640, 480))\n",
    "img1_raw = cv2.resize(img1_raw, (640, 480))\n",
    "\n",
    "img0 = torch.from_numpy(img0_raw)[None][None].cuda() / 255.\n",
    "img1 = torch.from_numpy(img1_raw)[None][None].cuda() / 255.\n",
    "batch = {'image0': img0, 'image1': img1}\n",
    "\n",
    "# Inference with LoFTR and get prediction\n",
    "with torch.no_grad():\n",
    "    matcher(batch)\n",
    "    mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "    mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "    mconf = batch['mconf'].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw\n",
    "color = cm.jet(mconf)\n",
    "text = [\n",
    "    'LoFTR',\n",
    "    'Matches: {}'.format(len(mkpts0)),\n",
    "]\n",
    "fig = make_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outdoor Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw\n",
    "color = cm.jet(mconf)\n",
    "text = [\n",
    "    'LoFTR',\n",
    "    'Matches: {}'.format(len(mkpts0)),\n",
    "]\n",
    "fig = make_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkpts1.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b8911f875a754a9ad2a8804064d078bf6a1985972bb0389b9d67771213c8e20"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
